{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4cd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roi Yehoshua\n",
    "# Date: January 2024\n",
    "# MIT License \n",
    "\n",
    "# Based on the PyTorch implementation from https://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a5dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17027ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fe1d1-df56-477d-9491-5f1896fe6387",
   "metadata": {},
   "source": [
    "### Multi-Head Attention \n",
    "\n",
    "1. **Head Calculation**: \n",
    "   $$\n",
    "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "   $$\n",
    " \n",
    "\n",
    "2. **Attention Function**: \n",
    "   $$\n",
    "   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "   $$\n",
    " \n",
    "\n",
    "3. **Concatenation and Final Projection**: \n",
    "   $$\n",
    "   \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf156177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"The multi-head attention module\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__() \n",
    "        \n",
    "        # Ensure the dimension of the model is divisible by the number of heads.\n",
    "        # This is necessary to equally divide the embedding dimension across heads.\n",
    "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "        \n",
    "        self.d_model = d_model           # Total dimension of the model\n",
    "        self.num_heads = num_heads       # Number of attention heads\n",
    "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
    "               \n",
    "        # Linear transformations for queries, keys, and values\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        \n",
    "        # 1. Calculate attention scores with scaling\n",
    "        # 2. Apply mask (if provided) by setting masked positions to a large negative value\n",
    "        # 3. Apply softmax to attention scores to get probabilities\n",
    "        # 4. Return the weighted sum of values based on attention probabilities\n",
    "        \n",
    "        d_k = Q.size(-1) \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax to get probabilities\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
    "        # to prepare for multi-head attention processing\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
    "        # [batch_size, seq_length, d_model]\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        \n",
    "        # 1. Linearly project the queries, keys, and values, and then split them into heads\n",
    "        # 2. Apply scaled dot-product attention for each head \n",
    "        # 3. Concatenate the heads' outputs and apply the final linear projection\n",
    "        # 1. Linearly project the queries, keys, and values, and then split them into heads\n",
    "        Q_proj = self.W_q(Q)\n",
    "        K_proj = self.W_k(K)\n",
    "        V_proj = self.W_v(V)\n",
    "        \n",
    "        Q_split = self.split_heads(Q_proj)\n",
    "        K_split = self.split_heads(K_proj)\n",
    "        V_split = self.split_heads(V_proj)\n",
    "        \n",
    "        # 2. Apply scaled dot-product attention for each head\n",
    "        attention_output = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask)\n",
    "        \n",
    "        # 3. Concatenate the heads' outputs\n",
    "        combined_output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # 4. Apply the final linear projection\n",
    "        output = self.W_o(combined_output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b13867",
   "metadata": {},
   "source": [
    "### Feed-Forward NN\n",
    "\n",
    "$$\n",
    "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5ab551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()       \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)        \n",
    "        self.linear2 = nn.Linear(d_ff, d_model)        \n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### WRITE YOUR CODE HERE        \n",
    "        return self.linear2(self.dropout(self.linear1(x).relu()))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034ef0a",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "$$\n",
    "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
    "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfca835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):    \n",
    "    \"\"\"\n",
    "    Implements the positional encoding module using sinusoidal functions of different frequencies \n",
    "    for each dimension of the encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()        \n",
    "        \n",
    "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
    "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        \n",
    "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the division term used in the formulas for sin and cos functions.\n",
    "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
    "        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
    "        # multiplying the position by the division term, creating a pattern where each position has\n",
    "        # a unique sinusoidal encoding.       \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
    "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
    "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor x.\n",
    "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
    "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
    "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2475b",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
    "       with a dropout, residual connection, and layer normalization after each sub-layer.    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        ### WRITE YOUR CODE HERE   \n",
    "        \n",
    "        # Apply self-attention to the input\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        \n",
    "        # Apply dropout to the attention output\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        \n",
    "        # Add the input x (residual connection) and apply layer normalization\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # Apply the feed-forward network to the output of the self-attention layer\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Apply dropout to the feed-forward network output\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        \n",
    "        # Add the output of the self-attention layer (residual connection) and apply layer normalization\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76215e02",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee27007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
    "       with a dropout, residual connection, and layer normalization after each sub-layer.    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)       \n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        \n",
    "        # Self-attention layer with target mask\n",
    "        # The target mask ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
    "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        self_attn_output = self.dropout(self_attn_output)\n",
    "        # Apply layer normalization after adding the residual connection\n",
    "        x = self.layer_norm1(x + self_attn_output)\n",
    "        \n",
    "        # Cross-attention layer with source mask\n",
    "        # The encoder output is used as the key and value in cross-attention to focus on relevant parts of the input sequence.\n",
    "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        cross_attn_output = self.dropout(cross_attn_output)\n",
    "        # Apply layer normalization after adding the residual connection\n",
    "        x = self.layer_norm2(x + cross_attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        # Apply layer normalization after adding the residual connection\n",
    "        x = self.layer_norm3(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad86614",
   "metadata": {},
   "source": [
    "### The Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31af1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, n_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers for source and target\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialization\n",
    "        self.init_weights()\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def create_source_mask(self, src):\n",
    "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"        \n",
    "        # Source padding mask\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
    "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
    "        # unsqueeze(2) adds a dimension for the attention scores \n",
    "        # This mask can be broadcasted across the src_len dimension of the attention scores, \n",
    "        # effectively masking out specific tokens across all heads and all positions in the sequence. \n",
    "        return src_mask    \n",
    "    \n",
    "    def create_target_mask(self, tgt):\n",
    "        # Target padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n",
    "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
    "        # unsqueeze(3) adds a dimension for the attention scores\n",
    "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only \n",
    "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
    "                \n",
    "        # Target no-peak mask\n",
    "        tgt_len = tgt.size(1)        \n",
    "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
    "        \n",
    "        # Combine masks\n",
    "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]        \n",
    "        return tgt_mask \n",
    "        \n",
    "    def encode(self, src):\n",
    "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
    "        \"\"\"       \n",
    "        src_mask = self.create_source_mask(src)\n",
    "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
    "        \n",
    "        # Pass through each layer in the encoder        \n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask)\n",
    "        return src, src_mask\n",
    "        \n",
    "    def decode(self, tgt, memory, src_mask):\n",
    "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
    "        \"\"\"\n",
    "        tgt_mask = self.create_target_mask(tgt)\n",
    "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
    "        \n",
    "        # Pass through each layer in the decoder\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.out(tgt)\n",
    "        return output\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        ### WRITE YOUR CODE HERE  \n",
    "\n",
    "        # Encode the source sequence\n",
    "        memory, src_mask = self.encode(src)\n",
    "        \n",
    "        # Decode the target sequence\n",
    "        output = self.decode(tgt, memory, src_mask)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a3b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = 5000  # Size of source vocabulary\n",
    "tgt_vocab_size = 5000  # Size of target vocabulary\n",
    "d_model = 512          # Embedding dimension\n",
    "N = 6                  # Number of encoder and decoder layers\n",
    "num_heads = 8          # Number of attention heads\n",
    "d_ff = 2048            # Dimension of feed forward networks\n",
    "max_seq_length = 100   # Maximum sequence length\n",
    "dropout = 0.1          # Dropout rate\n",
    "pad_idx = 0            # Index of the padding token\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f7248",
   "metadata": {},
   "source": [
    "### Testing on Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149dadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random sample data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b1c56",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45583975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([990], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the next token using the first token in the first target tensor\n",
    "model.eval()\n",
    "\n",
    "memory, src_mask = model.encode(src_data[:1, :])\n",
    "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
    "y = output.view(-1, tgt_vocab_size).argmax(-1)  \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7e1c5",
   "metadata": {},
   "source": [
    "If your code is correct, you should get tensor([990])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314148e",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2afd8ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.601297378540039\n",
      "Epoch: 2, Loss: 8.503870964050293\n",
      "Epoch: 3, Loss: 8.376736640930176\n",
      "Epoch: 4, Loss: 8.295574188232422\n",
      "Epoch: 5, Loss: 8.239795684814453\n",
      "Epoch: 6, Loss: 8.197484016418457\n",
      "Epoch: 7, Loss: 8.163225173950195\n",
      "Epoch: 8, Loss: 8.141745567321777\n",
      "Epoch: 9, Loss: 8.130845069885254\n",
      "Epoch: 10, Loss: 8.11706256866455\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 10 epochs\n",
    "grad_clip = 1.0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src_data, tgt_data[:, :-1])\n",
    "    \n",
    "    # tgt_data is of shape [batch_size, tgt_len]\n",
    "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
    "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
    "    loss = criterion(output, tgt)        \n",
    "    \n",
    "    loss.backward()        \n",
    "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)        \n",
    "    optimizer.step()    \n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f36f48",
   "metadata": {},
   "source": [
    "You should see the loss decreasing from around 8.6 to 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caa7ed",
   "metadata": {},
   "source": [
    "### Machine Translation Example\n",
    "\n",
    "Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. <br>\n",
    "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9a17e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this example make sure you have the following packages installed: \n",
    "#%pip install spacy torchtext portalocker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e67f4",
   "metadata": {},
   "source": [
    "#### Define Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f20abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy models for tokenization\n",
    "try:\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download de_core_news_sm\")\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "try:\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, language):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[language])\n",
    "\n",
    "tokenizer_de = get_tokenizer(tokenize_de)\n",
    "tokenizer_en = get_tokenizer(tokenize_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c9820",
   "metadata": {},
   "source": [
    "#### Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7474b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pat\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))\n",
    "vocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0), \n",
    "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1), \n",
    "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "vocab_src.set_default_index(vocab_src['<unk>'])\n",
    "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ca38f",
   "metadata": {},
   "source": [
    "#### Create the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c862170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
    "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
    "d_model = 512  # Embedding dimension\n",
    "N = 6          # Number of encoder and decoder layers\n",
    "num_heads = 8  # Number of attention heads\n",
    "d_ff = 2048    # Dimension of feed forward networks\n",
    "max_seq_length = 5000 # Maximum sequence length\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Assume pad_idx is the padding index in the target vocabulary\n",
    "pad_idx = vocab_tgt['<pad>']\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Hyperparameters for the training process\n",
    "batch_size = 128\n",
    "grad_clip = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa58a6e",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ca10d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_data_iter):\n",
    "    data = []\n",
    "    for raw_src, raw_tgt in raw_data_iter:\n",
    "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
    "        data.append((src_tensor, tgt_tensor))\n",
    "    return data\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
    "train_data = data_process(train_data)\n",
    "valid_data = data_process(valid_data)\n",
    "#test_data = data_process(test_data)   \n",
    "# The test set of Multi30k is corrupted\n",
    "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c2f452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
    "    to each sequence and padding all sequences to the same length.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
    "      containing the source sequence tensor and the target sequence tensor.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    src_batch, tgt_batch = [], []\n",
    "    \n",
    "    # Iterate over each source-target pair in the provided batch\n",
    "    for src_item, tgt_item in data_batch:\n",
    "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences        \n",
    "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item, \n",
    "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
    "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item, \n",
    "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
    "        \n",
    "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
    "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
    "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "# Similarly, DataLoader for the validation data\n",
    "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cbd8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch over the given dataset.\n",
    "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
    "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to be trained. \n",
    "    - iterator (Iterable): An iterable object that returns batches of data. \n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
    "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
    "    - grad_clip (float): The maximum norm of the gradients for gradient clipping. \n",
    "\n",
    "    Returns:\n",
    "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
    "    \"\"\"    \n",
    "    # Set the model to training mode. \n",
    "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
    "    model.train()   \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Enumerate over the data iterator to get batches\n",
    "    for i, batch in enumerate(iterator):         \n",
    "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the model. \n",
    "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        \n",
    "        # Reshape the output and target tensors to compute loss.\n",
    "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
    "                \n",
    "        # tgt is of shape [batch_size, tgt_len]\n",
    "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
    "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
    "        \n",
    "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        # Compute loss, perform backpropagation, and update model parameters\n",
    "        loss = criterion(output, tgt)          \n",
    "        loss.backward() \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  \n",
    "        optimizer.step()        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Compute average loss per batch for the current epoch\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31d18844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset.\n",
    "    This function is similar to the training loop, but without the backward pass and parameter updates. I\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])            \n",
    "            output_dim = output.shape[-1]            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287e5dd",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d02463b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "\tTrain Loss: 5.696\n",
      "\tVal Loss: 5.000\n",
      "\n",
      "Epoch: 2\n",
      "\tTrain Loss: 4.886\n",
      "\tVal Loss: 4.787\n",
      "\n",
      "Epoch: 3\n",
      "\tTrain Loss: 4.708\n",
      "\tVal Loss: 4.629\n",
      "\n",
      "Epoch: 4\n",
      "\tTrain Loss: 4.463\n",
      "\tVal Loss: 4.293\n",
      "\n",
      "Epoch: 5\n",
      "\tTrain Loss: 4.117\n",
      "\tVal Loss: 4.024\n",
      "\n",
      "Epoch: 6\n",
      "\tTrain Loss: 3.898\n",
      "\tVal Loss: 3.874\n",
      "\n",
      "Epoch: 7\n",
      "\tTrain Loss: 3.753\n",
      "\tVal Loss: 3.762\n",
      "\n",
      "Epoch: 8\n",
      "\tTrain Loss: 3.631\n",
      "\tVal Loss: 3.688\n",
      "\n",
      "Epoch: 9\n",
      "\tTrain Loss: 3.532\n",
      "\tVal Loss: 3.601\n",
      "\n",
      "Epoch: 10\n",
      "\tTrain Loss: 3.449\n",
      "\tVal Loss: 3.552\n",
      "\n",
      "Epoch: 11\n",
      "\tTrain Loss: 3.380\n",
      "\tVal Loss: 3.519\n",
      "\n",
      "Epoch: 12\n",
      "\tTrain Loss: 3.319\n",
      "\tVal Loss: 3.476\n",
      "\n",
      "Epoch: 13\n",
      "\tTrain Loss: 3.260\n",
      "\tVal Loss: 3.437\n",
      "\n",
      "Epoch: 14\n",
      "\tTrain Loss: 3.203\n",
      "\tVal Loss: 3.402\n",
      "\n",
      "Epoch: 15\n",
      "\tTrain Loss: 3.152\n",
      "\tVal Loss: 3.368\n",
      "\n",
      "Epoch: 16\n",
      "\tTrain Loss: 3.101\n",
      "\tVal Loss: 3.325\n",
      "\n",
      "Epoch: 17\n",
      "\tTrain Loss: 3.044\n",
      "\tVal Loss: 3.277\n",
      "\n",
      "Epoch: 18\n",
      "\tTrain Loss: 2.990\n",
      "\tVal Loss: 3.266\n",
      "\n",
      "Epoch: 19\n",
      "\tTrain Loss: 2.941\n",
      "\tVal Loss: 3.208\n",
      "\n",
      "Epoch: 20\n",
      "\tTrain Loss: 2.891\n",
      "\tVal Loss: 3.188\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
    "    val_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    print(f'\\nEpoch: {epoch + 1}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tVal Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a844bb",
   "metadata": {},
   "source": [
    "The train loss should decrease from around 5.7 to 2.8 after 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a7300",
   "metadata": {},
   "source": [
    "#### Translating a Sample Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b1bd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50):\n",
    "    \"\"\"\n",
    "    Translates a given source sentence into the target language using a trained Transformer model.\n",
    "    The function preprocesses the input sentence by tokenizing and converting it to tensor format, then uses the model's\n",
    "    encode and decode methods to generate the translated sentence. The translation process is performed token by token\n",
    "    using greedy decoding, selecting the most likely next token at each step until an <eos> token is produced or the\n",
    "    maximum length is reached.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained Transformer model. \n",
    "    - sentence (str): The source sentence to translate. \n",
    "    - vocab_src (dict): The source vocabulary mapping of tokens to indices. It should include special tokens such as\n",
    "      '<bos>' (beginning of sentence) and '<eos>' (end of sentence).\n",
    "    - vocab_tgt (dict): The target vocabulary mapping of indices to tokens. It should provide a method `lookup_token`\n",
    "      to convert token indices back to the string representation.\n",
    "    - max_length (int, optional): The maximum allowed length for the generated translation. The decoding process will\n",
    "      stop when this length is reached if an <eos> token has not yet been generated.\n",
    "\n",
    "    Returns:\n",
    "    - str: The translated sentence as a string of text in the target language.\n",
    "    \"\"\" \n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    # Determine the device to use based on the model's current allocation\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = [\"<bos>\"] + sentence.split() + [\"<eos>\"]\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    src_indices = [vocab_src[token] if token in vocab_src else vocab_src[\"<unk>\"] for token in tokens]\n",
    "    \n",
    "    # Convert list of indices to tensor and move to the correct device\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    \n",
    "    # Encode the source sentence\n",
    "    with torch.no_grad():\n",
    "        memory, src_mask = model.encode(src_tensor)\n",
    "    \n",
    "    # Initialize the target tensor with the index of <bos>, and move to the correct device\n",
    "    tgt_indices = [vocab_src[\"<bos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_tensor, memory, src_mask)\n",
    "        \n",
    "        # Select the index of the next word with the highest probability\n",
    "        next_word_idx = output[0, -1].argmax().item()\n",
    "        tgt_indices.append(next_word_idx)\n",
    "        \n",
    "        # Stop if <eos> is generated\n",
    "        if next_word_idx == vocab_tgt[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    # Convert indices to tokens\n",
    "    translated_tokens = [vocab_tgt.lookup_token(idx) for idx in tgt_indices[1:]]  # Skip <bos>\n",
    "    \n",
    "    # Join tokens to form the translated sentence\n",
    "    translated_sentence = ' '.join(translated_tokens)\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438aff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: A young boy is playing with a toy . <eos>\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
    "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
    "print(f'Translated sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6be352",
   "metadata": {},
   "source": [
    "You should get a translation similar to the reference after 20 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7d1dad5-88c8-474d-8afb-e8d7207db2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for lr=0.0001, batch_size=32, num_heads=4, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6064, Val Loss: 4.8829\n",
      "Epoch: 2, Train Loss: 4.8082, Val Loss: 4.7157\n",
      "Epoch: 3, Train Loss: 4.5243, Val Loss: 4.2982\n",
      "Epoch: 4, Train Loss: 4.1861, Val Loss: 4.1020\n",
      "Epoch: 5, Train Loss: 3.9683, Val Loss: 3.9001\n",
      "Epoch: 6, Train Loss: 3.7968, Val Loss: 3.7710\n",
      "Epoch: 7, Train Loss: 3.6536, Val Loss: 3.6897\n",
      "Epoch: 8, Train Loss: 3.5379, Val Loss: 3.5949\n",
      "Epoch: 9, Train Loss: 3.4373, Val Loss: 3.5158\n",
      "Epoch: 10, Train Loss: 3.3463, Val Loss: 3.4316\n",
      "Epoch: 11, Train Loss: 3.2578, Val Loss: 3.3806\n",
      "Epoch: 12, Train Loss: 3.1780, Val Loss: 3.2989\n",
      "Epoch: 13, Train Loss: 3.0957, Val Loss: 3.2303\n",
      "Epoch: 14, Train Loss: 3.0181, Val Loss: 3.1698\n",
      "Epoch: 15, Train Loss: 2.9429, Val Loss: 3.1108\n",
      "Epoch: 16, Train Loss: 2.8693, Val Loss: 3.0652\n",
      "Epoch: 17, Train Loss: 2.7963, Val Loss: 2.9988\n",
      "Epoch: 18, Train Loss: 2.7208, Val Loss: 2.9292\n",
      "Epoch: 19, Train Loss: 2.6416, Val Loss: 2.8751\n",
      "Epoch: 20, Train Loss: 2.5623, Val Loss: 2.7916\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=4, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.7183, Val Loss: 4.9801\n",
      "Epoch: 2, Train Loss: 4.8720, Val Loss: 4.8204\n",
      "Epoch: 3, Train Loss: 4.7031, Val Loss: 4.6335\n",
      "Epoch: 4, Train Loss: 4.4345, Val Loss: 4.2657\n",
      "Epoch: 5, Train Loss: 4.1157, Val Loss: 4.0431\n",
      "Epoch: 6, Train Loss: 3.9108, Val Loss: 3.9274\n",
      "Epoch: 7, Train Loss: 3.7607, Val Loss: 3.7700\n",
      "Epoch: 8, Train Loss: 3.6433, Val Loss: 3.7001\n",
      "Epoch: 9, Train Loss: 3.5457, Val Loss: 3.6175\n",
      "Epoch: 10, Train Loss: 3.4497, Val Loss: 3.5435\n",
      "Epoch: 11, Train Loss: 3.3669, Val Loss: 3.4862\n",
      "Epoch: 12, Train Loss: 3.2940, Val Loss: 3.4286\n",
      "Epoch: 13, Train Loss: 3.2268, Val Loss: 3.3929\n",
      "Epoch: 14, Train Loss: 3.1631, Val Loss: 3.3400\n",
      "Epoch: 15, Train Loss: 3.1059, Val Loss: 3.2953\n",
      "Epoch: 16, Train Loss: 3.0521, Val Loss: 3.2765\n",
      "Epoch: 17, Train Loss: 3.0012, Val Loss: 3.2276\n",
      "Epoch: 18, Train Loss: 2.9482, Val Loss: 3.1995\n",
      "Epoch: 19, Train Loss: 2.8956, Val Loss: 3.1756\n",
      "Epoch: 20, Train Loss: 2.8468, Val Loss: 3.1441\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=4, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.8823, Val Loss: 5.1902\n",
      "Epoch: 2, Train Loss: 5.1017, Val Loss: 5.0485\n",
      "Epoch: 3, Train Loss: 5.0019, Val Loss: 5.0131\n",
      "Epoch: 4, Train Loss: 4.9625, Val Loss: 4.9967\n",
      "Epoch: 5, Train Loss: 4.9369, Val Loss: 5.0347\n",
      "Epoch: 6, Train Loss: 4.9037, Val Loss: 4.9126\n",
      "Epoch: 7, Train Loss: 4.8816, Val Loss: 4.9482\n",
      "Epoch: 8, Train Loss: 4.7555, Val Loss: 4.8506\n",
      "Epoch: 9, Train Loss: 4.7125, Val Loss: 4.8744\n",
      "Epoch: 10, Train Loss: 4.6928, Val Loss: 4.9413\n",
      "Epoch: 11, Train Loss: 4.6611, Val Loss: 4.8351\n",
      "Epoch: 12, Train Loss: 4.6397, Val Loss: 4.7881\n",
      "Epoch: 13, Train Loss: 4.6263, Val Loss: 4.8249\n",
      "Epoch: 14, Train Loss: 4.6075, Val Loss: 4.7849\n",
      "Epoch: 15, Train Loss: 4.5928, Val Loss: 4.7592\n",
      "Epoch: 16, Train Loss: 4.5723, Val Loss: 4.7815\n",
      "Epoch: 17, Train Loss: 4.5555, Val Loss: 4.7314\n",
      "Epoch: 18, Train Loss: 4.5275, Val Loss: 4.7402\n",
      "Epoch: 19, Train Loss: 4.4932, Val Loss: 4.6735\n",
      "Epoch: 20, Train Loss: 4.4622, Val Loss: 4.6674\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=8, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6330, Val Loss: 4.9131\n",
      "Epoch: 2, Train Loss: 4.8080, Val Loss: 4.6651\n",
      "Epoch: 3, Train Loss: 4.4844, Val Loss: 4.2982\n",
      "Epoch: 4, Train Loss: 4.1626, Val Loss: 4.0501\n",
      "Epoch: 5, Train Loss: 3.9431, Val Loss: 3.9020\n",
      "Epoch: 6, Train Loss: 3.7831, Val Loss: 3.7795\n",
      "Epoch: 7, Train Loss: 3.6562, Val Loss: 3.6882\n",
      "Epoch: 8, Train Loss: 3.5425, Val Loss: 3.5972\n",
      "Epoch: 9, Train Loss: 3.4401, Val Loss: 3.5250\n",
      "Epoch: 10, Train Loss: 3.3421, Val Loss: 3.4423\n",
      "Epoch: 11, Train Loss: 3.2554, Val Loss: 3.3778\n",
      "Epoch: 12, Train Loss: 3.1753, Val Loss: 3.3209\n",
      "Epoch: 13, Train Loss: 3.0974, Val Loss: 3.2673\n",
      "Epoch: 14, Train Loss: 3.0229, Val Loss: 3.2064\n",
      "Epoch: 15, Train Loss: 2.9532, Val Loss: 3.1604\n",
      "Epoch: 16, Train Loss: 2.8857, Val Loss: 3.1053\n",
      "Epoch: 17, Train Loss: 2.8196, Val Loss: 3.0648\n",
      "Epoch: 18, Train Loss: 2.7508, Val Loss: 2.9999\n",
      "Epoch: 19, Train Loss: 2.6835, Val Loss: 2.9606\n",
      "Epoch: 20, Train Loss: 2.6147, Val Loss: 2.8898\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=8, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.7271, Val Loss: 5.0185\n",
      "Epoch: 2, Train Loss: 4.8962, Val Loss: 4.7970\n",
      "Epoch: 3, Train Loss: 4.7096, Val Loss: 4.6696\n",
      "Epoch: 4, Train Loss: 4.5073, Val Loss: 4.3581\n",
      "Epoch: 5, Train Loss: 4.1781, Val Loss: 4.0789\n",
      "Epoch: 6, Train Loss: 3.9451, Val Loss: 3.9086\n",
      "Epoch: 7, Train Loss: 3.7927, Val Loss: 3.8146\n",
      "Epoch: 8, Train Loss: 3.6691, Val Loss: 3.7177\n",
      "Epoch: 9, Train Loss: 3.5583, Val Loss: 3.6247\n",
      "Epoch: 10, Train Loss: 3.4660, Val Loss: 3.5608\n",
      "Epoch: 11, Train Loss: 3.3901, Val Loss: 3.5105\n",
      "Epoch: 12, Train Loss: 3.3166, Val Loss: 3.4390\n",
      "Epoch: 13, Train Loss: 3.2446, Val Loss: 3.4090\n",
      "Epoch: 14, Train Loss: 3.1786, Val Loss: 3.3523\n",
      "Epoch: 15, Train Loss: 3.1138, Val Loss: 3.3083\n",
      "Epoch: 16, Train Loss: 3.0535, Val Loss: 3.2652\n",
      "Epoch: 17, Train Loss: 2.9968, Val Loss: 3.2484\n",
      "Epoch: 18, Train Loss: 2.9471, Val Loss: 3.2004\n",
      "Epoch: 19, Train Loss: 2.8975, Val Loss: 3.1733\n",
      "Epoch: 20, Train Loss: 2.8473, Val Loss: 3.1640\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=8, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.8485, Val Loss: 5.1782\n",
      "Epoch: 2, Train Loss: 5.0933, Val Loss: 4.9524\n",
      "Epoch: 3, Train Loss: 4.8520, Val Loss: 4.8070\n",
      "Epoch: 4, Train Loss: 4.7180, Val Loss: 4.7326\n",
      "Epoch: 5, Train Loss: 4.5497, Val Loss: 4.4397\n",
      "Epoch: 6, Train Loss: 4.2603, Val Loss: 4.1738\n",
      "Epoch: 7, Train Loss: 4.0534, Val Loss: 4.0650\n",
      "Epoch: 8, Train Loss: 3.9386, Val Loss: 3.9982\n",
      "Epoch: 9, Train Loss: 3.8503, Val Loss: 3.9114\n",
      "Epoch: 10, Train Loss: 3.7637, Val Loss: 3.8247\n",
      "Epoch: 11, Train Loss: 3.6678, Val Loss: 3.7508\n",
      "Epoch: 12, Train Loss: 3.5862, Val Loss: 3.6956\n",
      "Epoch: 13, Train Loss: 3.5148, Val Loss: 3.6530\n",
      "Epoch: 14, Train Loss: 3.4452, Val Loss: 3.6100\n",
      "Epoch: 15, Train Loss: 3.3828, Val Loss: 3.5418\n",
      "Epoch: 16, Train Loss: 3.3209, Val Loss: 3.4891\n",
      "Epoch: 17, Train Loss: 3.2555, Val Loss: 3.4411\n",
      "Epoch: 18, Train Loss: 3.1945, Val Loss: 3.3951\n",
      "Epoch: 19, Train Loss: 3.1361, Val Loss: 3.3628\n",
      "Epoch: 20, Train Loss: 3.0817, Val Loss: 3.3353\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=16, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6401, Val Loss: 4.9220\n",
      "Epoch: 2, Train Loss: 4.8178, Val Loss: 4.6784\n",
      "Epoch: 3, Train Loss: 4.4962, Val Loss: 4.3028\n",
      "Epoch: 4, Train Loss: 4.1644, Val Loss: 4.0566\n",
      "Epoch: 5, Train Loss: 3.9614, Val Loss: 3.9467\n",
      "Epoch: 6, Train Loss: 3.7998, Val Loss: 3.8071\n",
      "Epoch: 7, Train Loss: 3.6738, Val Loss: 3.7368\n",
      "Epoch: 8, Train Loss: 3.5776, Val Loss: 3.6571\n",
      "Epoch: 9, Train Loss: 3.4885, Val Loss: 3.5683\n",
      "Epoch: 10, Train Loss: 3.4009, Val Loss: 3.5050\n",
      "Epoch: 11, Train Loss: 3.3190, Val Loss: 3.4456\n",
      "Epoch: 12, Train Loss: 3.2416, Val Loss: 3.3893\n",
      "Epoch: 13, Train Loss: 3.1685, Val Loss: 3.3527\n",
      "Epoch: 14, Train Loss: 3.1008, Val Loss: 3.2893\n",
      "Epoch: 15, Train Loss: 3.0356, Val Loss: 3.2399\n",
      "Epoch: 16, Train Loss: 2.9737, Val Loss: 3.1900\n",
      "Epoch: 17, Train Loss: 2.9145, Val Loss: 3.1655\n",
      "Epoch: 18, Train Loss: 2.8534, Val Loss: 3.1152\n",
      "Epoch: 19, Train Loss: 2.7907, Val Loss: 3.0636\n",
      "Epoch: 20, Train Loss: 2.7291, Val Loss: 3.0131\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=16, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.6965, Val Loss: 5.0557\n",
      "Epoch: 2, Train Loss: 4.8841, Val Loss: 4.7853\n",
      "Epoch: 3, Train Loss: 4.6751, Val Loss: 4.5591\n",
      "Epoch: 4, Train Loss: 4.3459, Val Loss: 4.1882\n",
      "Epoch: 5, Train Loss: 4.0604, Val Loss: 3.9951\n",
      "Epoch: 6, Train Loss: 3.8769, Val Loss: 3.8562\n",
      "Epoch: 7, Train Loss: 3.7403, Val Loss: 3.7587\n",
      "Epoch: 8, Train Loss: 3.6210, Val Loss: 3.6650\n",
      "Epoch: 9, Train Loss: 3.5201, Val Loss: 3.5959\n",
      "Epoch: 10, Train Loss: 3.4399, Val Loss: 3.5455\n",
      "Epoch: 11, Train Loss: 3.3742, Val Loss: 3.4967\n",
      "Epoch: 12, Train Loss: 3.3115, Val Loss: 3.4690\n",
      "Epoch: 13, Train Loss: 3.2463, Val Loss: 3.4098\n",
      "Epoch: 14, Train Loss: 3.1815, Val Loss: 3.3572\n",
      "Epoch: 15, Train Loss: 3.1191, Val Loss: 3.3172\n",
      "Epoch: 16, Train Loss: 3.0587, Val Loss: 3.2852\n",
      "Epoch: 17, Train Loss: 3.0022, Val Loss: 3.2490\n",
      "Epoch: 18, Train Loss: 2.9500, Val Loss: 3.2043\n",
      "Epoch: 19, Train Loss: 2.9014, Val Loss: 3.1857\n",
      "Epoch: 20, Train Loss: 2.8523, Val Loss: 3.1565\n",
      "Running experiment for lr=0.0001, batch_size=32, num_heads=16, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.8766, Val Loss: 5.1633\n",
      "Epoch: 2, Train Loss: 5.1221, Val Loss: 5.1752\n",
      "Epoch: 3, Train Loss: 5.0547, Val Loss: 5.0813\n",
      "Epoch: 4, Train Loss: 5.0092, Val Loss: 5.0886\n",
      "Epoch: 5, Train Loss: 4.9896, Val Loss: 5.0800\n",
      "Epoch: 6, Train Loss: 4.9714, Val Loss: 5.1130\n",
      "Epoch: 7, Train Loss: 4.9647, Val Loss: 5.1025\n",
      "Epoch: 8, Train Loss: 4.8761, Val Loss: 4.9503\n",
      "Epoch: 9, Train Loss: 4.7895, Val Loss: 4.9673\n",
      "Epoch: 10, Train Loss: 4.7106, Val Loss: 5.0898\n",
      "Epoch: 11, Train Loss: 4.6932, Val Loss: 5.0136\n",
      "Epoch: 12, Train Loss: 4.6336, Val Loss: 5.2005\n",
      "Epoch: 13, Train Loss: 4.6166, Val Loss: 5.3495\n",
      "Epoch: 14, Train Loss: 4.7381, Val Loss: 5.3310\n",
      "Epoch: 15, Train Loss: 4.9739, Val Loss: 12.4290\n",
      "Epoch: 16, Train Loss: 4.7312, Val Loss: 5.1107\n",
      "Epoch: 17, Train Loss: 4.5147, Val Loss: 5.0930\n",
      "Epoch: 18, Train Loss: 4.4661, Val Loss: 5.1522\n",
      "Epoch: 19, Train Loss: 4.4252, Val Loss: 5.0112\n",
      "Epoch: 20, Train Loss: 4.3853, Val Loss: 5.0129\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=4, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6113, Val Loss: 4.8747\n",
      "Epoch: 2, Train Loss: 4.8073, Val Loss: 4.7256\n",
      "Epoch: 3, Train Loss: 4.5394, Val Loss: 4.3242\n",
      "Epoch: 4, Train Loss: 4.1856, Val Loss: 4.0762\n",
      "Epoch: 5, Train Loss: 3.9553, Val Loss: 3.9011\n",
      "Epoch: 6, Train Loss: 3.7848, Val Loss: 3.7725\n",
      "Epoch: 7, Train Loss: 3.6406, Val Loss: 3.6692\n",
      "Epoch: 8, Train Loss: 3.5181, Val Loss: 3.5734\n",
      "Epoch: 9, Train Loss: 3.4114, Val Loss: 3.4818\n",
      "Epoch: 10, Train Loss: 3.3210, Val Loss: 3.4227\n",
      "Epoch: 11, Train Loss: 3.2370, Val Loss: 3.3430\n",
      "Epoch: 12, Train Loss: 3.1525, Val Loss: 3.2879\n",
      "Epoch: 13, Train Loss: 3.0747, Val Loss: 3.2400\n",
      "Epoch: 14, Train Loss: 2.9989, Val Loss: 3.1609\n",
      "Epoch: 15, Train Loss: 2.9281, Val Loss: 3.1206\n",
      "Epoch: 16, Train Loss: 2.8594, Val Loss: 3.0574\n",
      "Epoch: 17, Train Loss: 2.7882, Val Loss: 3.0066\n",
      "Epoch: 18, Train Loss: 2.7187, Val Loss: 2.9450\n",
      "Epoch: 19, Train Loss: 2.6499, Val Loss: 2.8871\n",
      "Epoch: 20, Train Loss: 2.5808, Val Loss: 2.8483\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=4, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.6962, Val Loss: 5.0167\n",
      "Epoch: 2, Train Loss: 4.8726, Val Loss: 4.7743\n",
      "Epoch: 3, Train Loss: 4.6968, Val Loss: 4.6467\n",
      "Epoch: 4, Train Loss: 4.4678, Val Loss: 4.3510\n",
      "Epoch: 5, Train Loss: 4.1517, Val Loss: 4.0523\n",
      "Epoch: 6, Train Loss: 3.9315, Val Loss: 3.9168\n",
      "Epoch: 7, Train Loss: 3.7729, Val Loss: 3.7691\n",
      "Epoch: 8, Train Loss: 3.6490, Val Loss: 3.7189\n",
      "Epoch: 9, Train Loss: 3.5522, Val Loss: 3.6187\n",
      "Epoch: 10, Train Loss: 3.4685, Val Loss: 3.5595\n",
      "Epoch: 11, Train Loss: 3.3907, Val Loss: 3.5080\n",
      "Epoch: 12, Train Loss: 3.3199, Val Loss: 3.4685\n",
      "Epoch: 13, Train Loss: 3.2513, Val Loss: 3.4264\n",
      "Epoch: 14, Train Loss: 3.1882, Val Loss: 3.3631\n",
      "Epoch: 15, Train Loss: 3.1293, Val Loss: 3.3297\n",
      "Epoch: 16, Train Loss: 3.0701, Val Loss: 3.2989\n",
      "Epoch: 17, Train Loss: 3.0162, Val Loss: 3.2484\n",
      "Epoch: 18, Train Loss: 2.9618, Val Loss: 3.2226\n",
      "Epoch: 19, Train Loss: 2.9138, Val Loss: 3.1836\n",
      "Epoch: 20, Train Loss: 2.8650, Val Loss: 3.1489\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=4, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.9609, Val Loss: 5.2340\n",
      "Epoch: 2, Train Loss: 5.1637, Val Loss: 5.1970\n",
      "Epoch: 3, Train Loss: 5.0853, Val Loss: 5.1511\n",
      "Epoch: 4, Train Loss: 5.0429, Val Loss: 5.0949\n",
      "Epoch: 5, Train Loss: 5.0158, Val Loss: 5.1351\n",
      "Epoch: 6, Train Loss: 5.0080, Val Loss: 5.1014\n",
      "Epoch: 7, Train Loss: 4.9658, Val Loss: 5.1291\n",
      "Epoch: 8, Train Loss: 4.9442, Val Loss: 5.1670\n",
      "Epoch: 9, Train Loss: 4.8113, Val Loss: 4.9996\n",
      "Epoch: 10, Train Loss: 4.7384, Val Loss: 5.1203\n",
      "Epoch: 11, Train Loss: 4.7338, Val Loss: 5.5413\n",
      "Epoch: 12, Train Loss: 4.6976, Val Loss: 5.4778\n",
      "Epoch: 13, Train Loss: 4.6669, Val Loss: 5.3016\n",
      "Epoch: 14, Train Loss: 4.6692, Val Loss: 5.7451\n",
      "Epoch: 15, Train Loss: 4.6605, Val Loss: 6.1467\n",
      "Epoch: 16, Train Loss: 4.6436, Val Loss: 6.0425\n",
      "Epoch: 17, Train Loss: 4.6223, Val Loss: 6.1547\n",
      "Epoch: 18, Train Loss: 4.6096, Val Loss: 6.3604\n",
      "Epoch: 19, Train Loss: 4.5945, Val Loss: 6.5006\n",
      "Epoch: 20, Train Loss: 4.5799, Val Loss: 6.0736\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=8, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6294, Val Loss: 4.9113\n",
      "Epoch: 2, Train Loss: 4.8047, Val Loss: 4.6832\n",
      "Epoch: 3, Train Loss: 4.4826, Val Loss: 4.2842\n",
      "Epoch: 4, Train Loss: 4.1493, Val Loss: 4.0412\n",
      "Epoch: 5, Train Loss: 3.9409, Val Loss: 3.8840\n",
      "Epoch: 6, Train Loss: 3.7652, Val Loss: 3.7832\n",
      "Epoch: 7, Train Loss: 3.6382, Val Loss: 3.6722\n",
      "Epoch: 8, Train Loss: 3.5382, Val Loss: 3.6070\n",
      "Epoch: 9, Train Loss: 3.4535, Val Loss: 3.5715\n",
      "Epoch: 10, Train Loss: 3.3690, Val Loss: 3.4766\n",
      "Epoch: 11, Train Loss: 3.2871, Val Loss: 3.4028\n",
      "Epoch: 12, Train Loss: 3.2101, Val Loss: 3.3470\n",
      "Epoch: 13, Train Loss: 3.1381, Val Loss: 3.3028\n",
      "Epoch: 14, Train Loss: 3.0656, Val Loss: 3.2582\n",
      "Epoch: 15, Train Loss: 2.9931, Val Loss: 3.1723\n",
      "Epoch: 16, Train Loss: 2.9216, Val Loss: 3.1534\n",
      "Epoch: 17, Train Loss: 2.8546, Val Loss: 3.0695\n",
      "Epoch: 18, Train Loss: 2.7842, Val Loss: 3.0144\n",
      "Epoch: 19, Train Loss: 2.7164, Val Loss: 2.9598\n",
      "Epoch: 20, Train Loss: 2.6453, Val Loss: 2.9113\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=8, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.6897, Val Loss: 5.0102\n",
      "Epoch: 2, Train Loss: 4.8794, Val Loss: 4.7917\n",
      "Epoch: 3, Train Loss: 4.6801, Val Loss: 4.5621\n",
      "Epoch: 4, Train Loss: 4.3910, Val Loss: 4.2963\n",
      "Epoch: 5, Train Loss: 4.1195, Val Loss: 4.0467\n",
      "Epoch: 6, Train Loss: 3.9262, Val Loss: 3.9106\n",
      "Epoch: 7, Train Loss: 3.7882, Val Loss: 3.8302\n",
      "Epoch: 8, Train Loss: 3.6650, Val Loss: 3.7004\n",
      "Epoch: 9, Train Loss: 3.5554, Val Loss: 3.6225\n",
      "Epoch: 10, Train Loss: 3.4687, Val Loss: 3.5716\n",
      "Epoch: 11, Train Loss: 3.3988, Val Loss: 3.5323\n",
      "Epoch: 12, Train Loss: 3.3314, Val Loss: 3.4892\n",
      "Epoch: 13, Train Loss: 3.2681, Val Loss: 3.4275\n",
      "Epoch: 14, Train Loss: 3.2009, Val Loss: 3.3660\n",
      "Epoch: 15, Train Loss: 3.1360, Val Loss: 3.3263\n",
      "Epoch: 16, Train Loss: 3.0727, Val Loss: 3.2743\n",
      "Epoch: 17, Train Loss: 3.0112, Val Loss: 3.2459\n",
      "Epoch: 18, Train Loss: 2.9565, Val Loss: 3.2102\n",
      "Epoch: 19, Train Loss: 2.9036, Val Loss: 3.1842\n",
      "Epoch: 20, Train Loss: 2.8526, Val Loss: 3.1453\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=8, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.9839, Val Loss: 5.6517\n",
      "Epoch: 2, Train Loss: 5.5209, Val Loss: 8.1245\n",
      "Epoch: 3, Train Loss: 5.2331, Val Loss: 10.3624\n",
      "Epoch: 4, Train Loss: 5.1060, Val Loss: 7.3300\n",
      "Epoch: 5, Train Loss: 4.9895, Val Loss: 5.3469\n",
      "Epoch: 6, Train Loss: 4.8319, Val Loss: 4.8307\n",
      "Epoch: 7, Train Loss: 4.6743, Val Loss: 4.7787\n",
      "Epoch: 8, Train Loss: 4.4990, Val Loss: 4.5792\n",
      "Epoch: 9, Train Loss: 4.2764, Val Loss: 4.2488\n",
      "Epoch: 10, Train Loss: 4.0623, Val Loss: 4.0792\n",
      "Epoch: 11, Train Loss: 3.9255, Val Loss: 3.9851\n",
      "Epoch: 12, Train Loss: 3.8282, Val Loss: 3.9081\n",
      "Epoch: 13, Train Loss: 3.7449, Val Loss: 3.8323\n",
      "Epoch: 14, Train Loss: 3.6763, Val Loss: 3.8071\n",
      "Epoch: 15, Train Loss: 3.6162, Val Loss: 3.7742\n",
      "Epoch: 16, Train Loss: 3.5589, Val Loss: 3.6999\n",
      "Epoch: 17, Train Loss: 3.5037, Val Loss: 3.6637\n",
      "Epoch: 18, Train Loss: 3.4509, Val Loss: 3.6336\n",
      "Epoch: 19, Train Loss: 3.4056, Val Loss: 3.5892\n",
      "Epoch: 20, Train Loss: 3.3613, Val Loss: 3.5662\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=16, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6275, Val Loss: 4.9233\n",
      "Epoch: 2, Train Loss: 4.8158, Val Loss: 4.6581\n",
      "Epoch: 3, Train Loss: 4.4618, Val Loss: 4.2360\n",
      "Epoch: 4, Train Loss: 4.1289, Val Loss: 4.0270\n",
      "Epoch: 5, Train Loss: 3.9254, Val Loss: 3.8933\n",
      "Epoch: 6, Train Loss: 3.7678, Val Loss: 3.7727\n",
      "Epoch: 7, Train Loss: 3.6407, Val Loss: 3.6780\n",
      "Epoch: 8, Train Loss: 3.5401, Val Loss: 3.6010\n",
      "Epoch: 9, Train Loss: 3.4454, Val Loss: 3.5267\n",
      "Epoch: 10, Train Loss: 3.3586, Val Loss: 3.4661\n",
      "Epoch: 11, Train Loss: 3.2791, Val Loss: 3.4116\n",
      "Epoch: 12, Train Loss: 3.2017, Val Loss: 3.3473\n",
      "Epoch: 13, Train Loss: 3.1310, Val Loss: 3.3082\n",
      "Epoch: 14, Train Loss: 3.0671, Val Loss: 3.2517\n",
      "Epoch: 15, Train Loss: 3.0044, Val Loss: 3.2210\n",
      "Epoch: 16, Train Loss: 2.9364, Val Loss: 3.1568\n",
      "Epoch: 17, Train Loss: 2.8745, Val Loss: 3.1123\n",
      "Epoch: 18, Train Loss: 2.8103, Val Loss: 3.0776\n",
      "Epoch: 19, Train Loss: 2.7473, Val Loss: 3.0177\n",
      "Epoch: 20, Train Loss: 2.6805, Val Loss: 2.9707\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=16, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.7187, Val Loss: 5.0163\n",
      "Epoch: 2, Train Loss: 4.8728, Val Loss: 4.7743\n",
      "Epoch: 3, Train Loss: 4.6507, Val Loss: 4.5039\n",
      "Epoch: 4, Train Loss: 4.3176, Val Loss: 4.1582\n",
      "Epoch: 5, Train Loss: 4.0203, Val Loss: 3.9431\n",
      "Epoch: 6, Train Loss: 3.8347, Val Loss: 3.8275\n",
      "Epoch: 7, Train Loss: 3.7036, Val Loss: 3.7439\n",
      "Epoch: 8, Train Loss: 3.5896, Val Loss: 3.6440\n",
      "Epoch: 9, Train Loss: 3.5006, Val Loss: 3.5851\n",
      "Epoch: 10, Train Loss: 3.4227, Val Loss: 3.5268\n",
      "Epoch: 11, Train Loss: 3.3549, Val Loss: 3.4780\n",
      "Epoch: 12, Train Loss: 3.2917, Val Loss: 3.4485\n",
      "Epoch: 13, Train Loss: 3.2324, Val Loss: 3.3905\n",
      "Epoch: 14, Train Loss: 3.1739, Val Loss: 3.3743\n",
      "Epoch: 15, Train Loss: 3.1111, Val Loss: 3.3134\n",
      "Epoch: 16, Train Loss: 3.0565, Val Loss: 3.2811\n",
      "Epoch: 17, Train Loss: 3.0027, Val Loss: 3.2542\n",
      "Epoch: 18, Train Loss: 2.9524, Val Loss: 3.2207\n",
      "Epoch: 19, Train Loss: 2.9011, Val Loss: 3.1751\n",
      "Epoch: 20, Train Loss: 2.8554, Val Loss: 3.1686\n",
      "Running experiment for lr=0.0001, batch_size=64, num_heads=16, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.9010, Val Loss: 5.2289\n",
      "Epoch: 2, Train Loss: 5.1196, Val Loss: 5.0869\n",
      "Epoch: 3, Train Loss: 5.0488, Val Loss: 5.1367\n",
      "Epoch: 4, Train Loss: 5.0096, Val Loss: 5.1536\n",
      "Epoch: 5, Train Loss: 4.9893, Val Loss: 5.0806\n",
      "Epoch: 6, Train Loss: 4.9792, Val Loss: 5.0811\n",
      "Epoch: 7, Train Loss: 4.9588, Val Loss: 5.0864\n",
      "Epoch: 8, Train Loss: 4.8697, Val Loss: 5.0860\n",
      "Epoch: 9, Train Loss: 4.7779, Val Loss: 5.0707\n",
      "Epoch: 10, Train Loss: 4.7124, Val Loss: 5.1328\n",
      "Epoch: 11, Train Loss: 4.6616, Val Loss: 5.2788\n",
      "Epoch: 12, Train Loss: 4.6349, Val Loss: 5.1757\n",
      "Epoch: 13, Train Loss: 4.6071, Val Loss: 5.1967\n",
      "Epoch: 14, Train Loss: 4.6352, Val Loss: 5.7996\n",
      "Epoch: 15, Train Loss: 4.6450, Val Loss: 6.3853\n",
      "Epoch: 16, Train Loss: 4.6062, Val Loss: 5.6078\n",
      "Epoch: 17, Train Loss: 4.5571, Val Loss: 5.8963\n",
      "Epoch: 18, Train Loss: 4.5225, Val Loss: 5.7296\n",
      "Epoch: 19, Train Loss: 4.4918, Val Loss: 5.7522\n",
      "Epoch: 20, Train Loss: 4.4818, Val Loss: 5.9864\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=4, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.5929, Val Loss: 4.8587\n",
      "Epoch: 2, Train Loss: 4.7758, Val Loss: 4.6349\n",
      "Epoch: 3, Train Loss: 4.4632, Val Loss: 4.3160\n",
      "Epoch: 4, Train Loss: 4.1528, Val Loss: 4.0501\n",
      "Epoch: 5, Train Loss: 3.9417, Val Loss: 3.8966\n",
      "Epoch: 6, Train Loss: 3.7821, Val Loss: 3.7854\n",
      "Epoch: 7, Train Loss: 3.6495, Val Loss: 3.6842\n",
      "Epoch: 8, Train Loss: 3.5254, Val Loss: 3.5783\n",
      "Epoch: 9, Train Loss: 3.4104, Val Loss: 3.4787\n",
      "Epoch: 10, Train Loss: 3.3060, Val Loss: 3.4060\n",
      "Epoch: 11, Train Loss: 3.2128, Val Loss: 3.3236\n",
      "Epoch: 12, Train Loss: 3.1233, Val Loss: 3.2420\n",
      "Epoch: 13, Train Loss: 3.0410, Val Loss: 3.1809\n",
      "Epoch: 14, Train Loss: 2.9598, Val Loss: 3.1159\n",
      "Epoch: 15, Train Loss: 2.8785, Val Loss: 3.0656\n",
      "Epoch: 16, Train Loss: 2.7974, Val Loss: 2.9721\n",
      "Epoch: 17, Train Loss: 2.7177, Val Loss: 2.9258\n",
      "Epoch: 18, Train Loss: 2.6372, Val Loss: 2.8508\n",
      "Epoch: 19, Train Loss: 2.5550, Val Loss: 2.7845\n",
      "Epoch: 20, Train Loss: 2.4755, Val Loss: 2.7324\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=4, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.6970, Val Loss: 5.0093\n",
      "Epoch: 2, Train Loss: 4.8686, Val Loss: 4.7881\n",
      "Epoch: 3, Train Loss: 4.7028, Val Loss: 4.6625\n",
      "Epoch: 4, Train Loss: 4.4498, Val Loss: 4.2903\n",
      "Epoch: 5, Train Loss: 4.1386, Val Loss: 4.0425\n",
      "Epoch: 6, Train Loss: 3.9373, Val Loss: 3.9021\n",
      "Epoch: 7, Train Loss: 3.7802, Val Loss: 3.8011\n",
      "Epoch: 8, Train Loss: 3.6670, Val Loss: 3.7401\n",
      "Epoch: 9, Train Loss: 3.5710, Val Loss: 3.6474\n",
      "Epoch: 10, Train Loss: 3.4825, Val Loss: 3.5691\n",
      "Epoch: 11, Train Loss: 3.3952, Val Loss: 3.5128\n",
      "Epoch: 12, Train Loss: 3.3160, Val Loss: 3.4511\n",
      "Epoch: 13, Train Loss: 3.2469, Val Loss: 3.4013\n",
      "Epoch: 14, Train Loss: 3.1828, Val Loss: 3.3396\n",
      "Epoch: 15, Train Loss: 3.1219, Val Loss: 3.3159\n",
      "Epoch: 16, Train Loss: 3.0649, Val Loss: 3.2742\n",
      "Epoch: 17, Train Loss: 3.0098, Val Loss: 3.2383\n",
      "Epoch: 18, Train Loss: 2.9611, Val Loss: 3.2129\n",
      "Epoch: 19, Train Loss: 2.9100, Val Loss: 3.1648\n",
      "Epoch: 20, Train Loss: 2.8604, Val Loss: 3.1306\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=4, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.8835, Val Loss: 5.1714\n",
      "Epoch: 2, Train Loss: 5.1427, Val Loss: 5.0933\n",
      "Epoch: 3, Train Loss: 5.0651, Val Loss: 5.0764\n",
      "Epoch: 4, Train Loss: 5.0302, Val Loss: 5.1079\n",
      "Epoch: 5, Train Loss: 5.0057, Val Loss: 5.0876\n",
      "Epoch: 6, Train Loss: 4.9781, Val Loss: 5.0838\n",
      "Epoch: 7, Train Loss: 4.8977, Val Loss: 5.0027\n",
      "Epoch: 8, Train Loss: 4.7868, Val Loss: 4.9238\n",
      "Epoch: 9, Train Loss: 4.7218, Val Loss: 5.0768\n",
      "Epoch: 10, Train Loss: 4.6809, Val Loss: 5.0336\n",
      "Epoch: 11, Train Loss: 4.6506, Val Loss: 5.1991\n",
      "Epoch: 12, Train Loss: 4.6217, Val Loss: 5.1344\n",
      "Epoch: 13, Train Loss: 4.5882, Val Loss: 5.1614\n",
      "Epoch: 14, Train Loss: 4.5441, Val Loss: 5.0523\n",
      "Epoch: 15, Train Loss: 4.5171, Val Loss: 5.1220\n",
      "Epoch: 16, Train Loss: 4.4902, Val Loss: 5.1627\n",
      "Epoch: 17, Train Loss: 4.4628, Val Loss: 5.0442\n",
      "Epoch: 18, Train Loss: 4.4348, Val Loss: 4.9466\n",
      "Epoch: 19, Train Loss: 4.4075, Val Loss: 4.9613\n",
      "Epoch: 20, Train Loss: 4.3794, Val Loss: 4.9411\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=8, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6306, Val Loss: 4.8787\n",
      "Epoch: 2, Train Loss: 4.7859, Val Loss: 4.6335\n",
      "Epoch: 3, Train Loss: 4.4647, Val Loss: 4.3128\n",
      "Epoch: 4, Train Loss: 4.1825, Val Loss: 4.0802\n",
      "Epoch: 5, Train Loss: 3.9636, Val Loss: 3.9249\n",
      "Epoch: 6, Train Loss: 3.8092, Val Loss: 3.8332\n",
      "Epoch: 7, Train Loss: 3.6904, Val Loss: 3.7566\n",
      "Epoch: 8, Train Loss: 3.5870, Val Loss: 3.6801\n",
      "Epoch: 9, Train Loss: 3.4840, Val Loss: 3.5562\n",
      "Epoch: 10, Train Loss: 3.3845, Val Loss: 3.4721\n",
      "Epoch: 11, Train Loss: 3.2899, Val Loss: 3.4186\n",
      "Epoch: 12, Train Loss: 3.2052, Val Loss: 3.3313\n",
      "Epoch: 13, Train Loss: 3.1239, Val Loss: 3.2608\n",
      "Epoch: 14, Train Loss: 3.0412, Val Loss: 3.2067\n",
      "Epoch: 15, Train Loss: 2.9624, Val Loss: 3.1443\n",
      "Epoch: 16, Train Loss: 2.8817, Val Loss: 3.0645\n",
      "Epoch: 17, Train Loss: 2.8012, Val Loss: 2.9964\n",
      "Epoch: 18, Train Loss: 2.7209, Val Loss: 2.9198\n",
      "Epoch: 19, Train Loss: 2.6362, Val Loss: 2.8508\n",
      "Epoch: 20, Train Loss: 2.5575, Val Loss: 2.7902\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=8, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.7061, Val Loss: 5.0060\n",
      "Epoch: 2, Train Loss: 4.8950, Val Loss: 4.7791\n",
      "Epoch: 3, Train Loss: 4.6888, Val Loss: 4.6031\n",
      "Epoch: 4, Train Loss: 4.3918, Val Loss: 4.2088\n",
      "Epoch: 5, Train Loss: 4.0771, Val Loss: 4.0023\n",
      "Epoch: 6, Train Loss: 3.8891, Val Loss: 3.9007\n",
      "Epoch: 7, Train Loss: 3.7593, Val Loss: 3.7832\n",
      "Epoch: 8, Train Loss: 3.6457, Val Loss: 3.6916\n",
      "Epoch: 9, Train Loss: 3.5424, Val Loss: 3.6434\n",
      "Epoch: 10, Train Loss: 3.4601, Val Loss: 3.5680\n",
      "Epoch: 11, Train Loss: 3.3890, Val Loss: 3.5142\n",
      "Epoch: 12, Train Loss: 3.3198, Val Loss: 3.4568\n",
      "Epoch: 13, Train Loss: 3.2517, Val Loss: 3.4208\n",
      "Epoch: 14, Train Loss: 3.1876, Val Loss: 3.3675\n",
      "Epoch: 15, Train Loss: 3.1282, Val Loss: 3.3272\n",
      "Epoch: 16, Train Loss: 3.0685, Val Loss: 3.2866\n",
      "Epoch: 17, Train Loss: 3.0112, Val Loss: 3.2562\n",
      "Epoch: 18, Train Loss: 2.9592, Val Loss: 3.2316\n",
      "Epoch: 19, Train Loss: 2.9093, Val Loss: 3.2059\n",
      "Epoch: 20, Train Loss: 2.8641, Val Loss: 3.1713\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=8, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.9128, Val Loss: 5.2092\n",
      "Epoch: 2, Train Loss: 5.1823, Val Loss: 5.1503\n",
      "Epoch: 3, Train Loss: 5.1164, Val Loss: 5.2354\n",
      "Epoch: 4, Train Loss: 5.0867, Val Loss: 5.1572\n",
      "Epoch: 5, Train Loss: 5.0604, Val Loss: 5.1529\n",
      "Epoch: 6, Train Loss: 5.0162, Val Loss: 5.2529\n",
      "Epoch: 7, Train Loss: 4.9409, Val Loss: 5.4005\n",
      "Epoch: 8, Train Loss: 4.8397, Val Loss: 5.4941\n",
      "Epoch: 9, Train Loss: 4.7661, Val Loss: 5.4837\n",
      "Epoch: 10, Train Loss: 4.7137, Val Loss: 5.3862\n",
      "Epoch: 11, Train Loss: 4.6800, Val Loss: 5.4385\n",
      "Epoch: 12, Train Loss: 4.6592, Val Loss: 5.7086\n",
      "Epoch: 13, Train Loss: 4.6349, Val Loss: 5.6094\n",
      "Epoch: 14, Train Loss: 4.6236, Val Loss: 5.1882\n",
      "Epoch: 15, Train Loss: 4.5815, Val Loss: 5.1524\n",
      "Epoch: 16, Train Loss: 4.5486, Val Loss: 5.1627\n",
      "Epoch: 17, Train Loss: 4.5112, Val Loss: 5.1608\n",
      "Epoch: 18, Train Loss: 4.4784, Val Loss: 5.0408\n",
      "Epoch: 19, Train Loss: 4.5272, Val Loss: 5.7257\n",
      "Epoch: 20, Train Loss: 4.4700, Val Loss: 5.8359\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=16, num_layers=4\n",
      "Epoch: 1, Train Loss: 5.6413, Val Loss: 4.9335\n",
      "Epoch: 2, Train Loss: 4.8099, Val Loss: 4.6625\n",
      "Epoch: 3, Train Loss: 4.4547, Val Loss: 4.2545\n",
      "Epoch: 4, Train Loss: 4.1472, Val Loss: 4.0527\n",
      "Epoch: 5, Train Loss: 3.9535, Val Loss: 3.9046\n",
      "Epoch: 6, Train Loss: 3.7911, Val Loss: 3.7942\n",
      "Epoch: 7, Train Loss: 3.6691, Val Loss: 3.7112\n",
      "Epoch: 8, Train Loss: 3.5719, Val Loss: 3.6438\n",
      "Epoch: 9, Train Loss: 3.4770, Val Loss: 3.5581\n",
      "Epoch: 10, Train Loss: 3.3816, Val Loss: 3.4864\n",
      "Epoch: 11, Train Loss: 3.2966, Val Loss: 3.4230\n",
      "Epoch: 12, Train Loss: 3.2141, Val Loss: 3.3799\n",
      "Epoch: 13, Train Loss: 3.1420, Val Loss: 3.3174\n",
      "Epoch: 14, Train Loss: 3.0726, Val Loss: 3.2598\n",
      "Epoch: 15, Train Loss: 3.0067, Val Loss: 3.2183\n",
      "Epoch: 16, Train Loss: 2.9462, Val Loss: 3.1698\n",
      "Epoch: 17, Train Loss: 2.8890, Val Loss: 3.1360\n",
      "Epoch: 18, Train Loss: 2.8277, Val Loss: 3.1048\n",
      "Epoch: 19, Train Loss: 2.7739, Val Loss: 3.0643\n",
      "Epoch: 20, Train Loss: 2.7145, Val Loss: 3.0075\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=16, num_layers=6\n",
      "Epoch: 1, Train Loss: 5.7155, Val Loss: 5.0000\n",
      "Epoch: 2, Train Loss: 4.8730, Val Loss: 4.7675\n",
      "Epoch: 3, Train Loss: 4.6248, Val Loss: 4.4582\n",
      "Epoch: 4, Train Loss: 4.3049, Val Loss: 4.1781\n",
      "Epoch: 5, Train Loss: 4.0395, Val Loss: 3.9743\n",
      "Epoch: 6, Train Loss: 3.8563, Val Loss: 3.8551\n",
      "Epoch: 7, Train Loss: 3.7296, Val Loss: 3.7534\n",
      "Epoch: 8, Train Loss: 3.6170, Val Loss: 3.6674\n",
      "Epoch: 9, Train Loss: 3.5194, Val Loss: 3.5968\n",
      "Epoch: 10, Train Loss: 3.4404, Val Loss: 3.5447\n",
      "Epoch: 11, Train Loss: 3.3742, Val Loss: 3.5077\n",
      "Epoch: 12, Train Loss: 3.3108, Val Loss: 3.4690\n",
      "Epoch: 13, Train Loss: 3.2485, Val Loss: 3.4177\n",
      "Epoch: 14, Train Loss: 3.1928, Val Loss: 3.3704\n",
      "Epoch: 15, Train Loss: 3.1372, Val Loss: 3.3257\n",
      "Epoch: 16, Train Loss: 3.0803, Val Loss: 3.3016\n",
      "Epoch: 17, Train Loss: 3.0242, Val Loss: 3.2700\n",
      "Epoch: 18, Train Loss: 2.9749, Val Loss: 3.2204\n",
      "Epoch: 19, Train Loss: 2.9249, Val Loss: 3.1992\n",
      "Epoch: 20, Train Loss: 2.8794, Val Loss: 3.1854\n",
      "Running experiment for lr=0.0001, batch_size=128, num_heads=16, num_layers=8\n",
      "Epoch: 1, Train Loss: 5.9437, Val Loss: 5.2302\n",
      "Epoch: 2, Train Loss: 5.1789, Val Loss: 5.1509\n",
      "Epoch: 3, Train Loss: 5.1123, Val Loss: 5.1448\n",
      "Epoch: 4, Train Loss: 5.0750, Val Loss: 5.1821\n",
      "Epoch: 5, Train Loss: 5.0279, Val Loss: 5.3477\n",
      "Epoch: 6, Train Loss: 4.9319, Val Loss: 5.5670\n",
      "Epoch: 7, Train Loss: 4.9016, Val Loss: 5.6524\n",
      "Epoch: 8, Train Loss: 4.8700, Val Loss: 5.5321\n",
      "Epoch: 9, Train Loss: 4.8015, Val Loss: 5.8913\n",
      "Epoch: 10, Train Loss: 4.7437, Val Loss: 5.5854\n",
      "Epoch: 11, Train Loss: 4.6846, Val Loss: 6.2932\n",
      "Epoch: 12, Train Loss: 4.6796, Val Loss: 6.3886\n",
      "Epoch: 13, Train Loss: 4.6462, Val Loss: 6.3926\n",
      "Epoch: 14, Train Loss: 4.8384, Val Loss: 12.7981\n",
      "Epoch: 15, Train Loss: 4.7963, Val Loss: 12.6457\n",
      "Epoch: 16, Train Loss: 4.7613, Val Loss: 12.7771\n",
      "Epoch: 17, Train Loss: 4.7242, Val Loss: 12.7865\n",
      "Epoch: 18, Train Loss: 4.6942, Val Loss: 13.4215\n",
      "Epoch: 19, Train Loss: 4.6794, Val Loss: 13.4803\n",
      "Epoch: 20, Train Loss: 4.6389, Val Loss: 13.2562\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to run an experiment with given hyperparameters\n",
    "def run_experiment(learning_rate, batch_size, num_heads, num_layers):\n",
    "    # Initialize the model with specified hyperparameters\n",
    "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
    "        val_loss = evaluate(model, valid_iterator, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Define hyperparameter settings to experiment with\n",
    "learning_rates = [0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "num_heads_list = [4, 8, 16]\n",
    "num_layers_list = [4, 6, 8]\n",
    "\n",
    "# Run experiments for different combinations of hyperparameters\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for num_heads in num_heads_list:\n",
    "            for num_layers in num_layers_list:\n",
    "                print(f'Running experiment for lr={lr}, batch_size={batch_size}, num_heads={num_heads}, num_layers={num_layers}')\n",
    "                train_losses, val_losses = run_experiment(lr, batch_size, num_heads, num_layers)\n",
    "                results[(lr, batch_size, num_heads, num_layers)] = (train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9b60a-7089-4b09-808d-a4fe90713edd",
   "metadata": {},
   "source": [
    "config with lowest loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4f2a44e-fd7d-40cd-b35d-1cd78a978b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.0001, 128, 4, 4), 2.475543434924491, 2.7323555648326874)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "min_loss_key = None\n",
    "min_train_loss_value = float('inf')\n",
    "min_val_loss_value = float('inf')\n",
    "\n",
    "for key, (train_losses, val_losses) in results.items():\n",
    "    if train_losses[-1] < min_train_loss_value or val_losses[-1] < min_val_loss_value:\n",
    "        if train_losses[-1] < min_train_loss_value:\n",
    "            min_train_loss_value = train_losses[-1]\n",
    "        if val_losses[-1] < min_val_loss_value:\n",
    "            min_val_loss_value = val_losses[-1]\n",
    "        min_loss_key = key\n",
    "\n",
    "min_loss_key, min_train_loss_value, min_val_loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97356edf-975d-425d-834f-cf233dacd925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
